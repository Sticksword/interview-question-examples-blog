{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Prompt: Given a non-empty message string containing only digits, determine the total number of ways to decode it.\n",
    "\n",
    "The message containing letters from A-Z is being encoded to numbers using the following mapping:\n",
    "\n",
    "'A' -> 1, 'B' -> 2, ...'Z' -> 26\n",
    "\n",
    "#### Example 1:\n",
    "Input: \"12\"\n",
    "Output: 2\n",
    "Explanation: It could be decoded as \"AB\" (1 2) or \"L\" (12).\n",
    " \n",
    "#### Example 2:\n",
    "Input: \"226\"\n",
    "Output: 3\n",
    "Explanation: It could be decoded as \"BZ\" (2 26), \"VF\" (22 6), or \"BBF\" (2 2 6).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "  # def __init__(self, numDigits):\n",
    "    # self.storage = [None] * numDigits\n",
    "        \n",
    "  def solve(self, digits: str):\n",
    "    if len(digits) == 1:\n",
    "      return 1\n",
    "    \n",
    "    toAdd = 0\n",
    "    prior = digits[-2]\n",
    "    if prior == '1' or (prior == '2' and int(digits[-1]) < 7):\n",
    "      toAdd = 1\n",
    "        \n",
    "    return self.solve(digits[0: len(digits) - 1]) + toAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "s = Solution()\n",
    "print(s.solve('12')) # 2\n",
    "\n",
    "s = Solution()\n",
    "print(s.solve('226')) # 3\n",
    "\n",
    "s = Solution()\n",
    "print(s.solve('1212')) # 4\n",
    "\n",
    "s = Solution()\n",
    "print(s.solve('123456')) # 3\n",
    "\n",
    "s = Solution()\n",
    "print(s.solve('1')) # 1\n",
    "\n",
    "s = Solution()\n",
    "print(s.solve('227')) # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * Machine Learning - Design Questions * * * * *\n",
    "The following questions are for machine learning engineer roles. I have gone through a few during these last 6 months and figured it would be good to share and document. They are system design questions in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Machine Learning for Credit Risk Evaluation\n",
    "## Prompt: How do you test whether a new credit risk scoring model works? What data would you look at?\n",
    "\n",
    "#### Initial Thoughts\n",
    "First, I would like to take a step back and ask what kind of model are we using.\n",
    "\n",
    "It seems like this is a supervised learning problem so we would need some kind of supervised learning model. Let's first focus on the second half of the question: \"What data would you look at?\"\n",
    "\n",
    "#### The Data\n",
    "The data could consist of a bunch of details on a person. The label for each data record would be whether or not they defaulted. In this way, if we train a model on this data, we are trying to establish a causal relationship between various details on a person and the likelihood they will default.\n",
    "\n",
    "#### Model\n",
    "Model wise, I don't see much reason to pick say XGBoost over Random Forests. We can try both and see how the metrics end up being for each. Depending on how much we know about the data and the assumptions we can make, perhaps picking a non-parametric model would be better, in order to fully learn from the data. If the cross validation scoring is worse and we overfit, we can always apply regularization constraints to the non-parametric model, such as limiting the max depth for Random Forests.\n",
    "\n",
    "Another thing to consider is would this be a classification task or a regression task. If the decision was to classify a user profile into \"risky\" vs \"not risky\", then this would be a surefire classification task. However, we probably want to play around with the probabilities and trade off precision and recall. In scikit learn, classifiers provide a handy `predict_proba` method to output the probabilities behind a certain classification. Through this, you can adjust the decision threshold and thus adjust the precision/recall accordingly.\n",
    "\n",
    "Stepping back to the original question, we can evaluate how well our model works by comparing to some benchmark that's already established in the industry. Perhaps we want to aim for a 0.8 F1 score. In production, we can check the output frequencies to verify the model is working properly. We can also randomly sample the output of the model in production and manually check if the decision is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Machine Learning for Customer Support\n",
    "## Prompt: Automate some portion of the customer support experience\n",
    "\n",
    "#### Initial Thoughts\n",
    "This also feels like a supervised problem. It seems like we would want to have some kind of article output mechanism given some text input.\n",
    "\n",
    "#### Training Data:\n",
    "I'd imagine the training data to be the single article exchanges between a customer and a customer support representative. If we wanted, we can add in app behavioral features to supplement the text input data.\n",
    "\n",
    "```\n",
    "Customer: How do I buy Bitcoin?\n",
    "CS representative: Here's a link to a support article on how to buy Bitcoin.\n",
    "Customer: Thanks!\n",
    "```\n",
    "\n",
    "Here we can assume the link provided was enough and the exchange ends. Thus our training data would comprise of these mappings between text input and an article selection output.\n",
    "\n",
    "\"How do I buy Bitcoin?\" → article 102 about buying Bitcoin on the app\n",
    "\"Where can I do X\" → article 59 about X\n",
    "...\n",
    "\n",
    "#### Approach\n",
    "Let's convert the text input into word vectors - the industry standard `word2vec` format will do.\n",
    "\n",
    "#### Models\n",
    "We can use a traditional Random Forest or we can use deep learning by building a large deep neural network.\n",
    "Either way, we would want to regularize because both tend to overfit if we are not careful.\n",
    "\n",
    "#### Output\n",
    "We can approach this by doing multinomial classification.\n",
    "\n",
    "#### Evaluation\n",
    "We can see for a given model, how many times does a customer respond, and in particular how many times do they respond with negative feedback. Alternatively, we can provide a yes button and a no button with a label that says \"Was the  article useful?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Machine Learning for Fulfillment Center Optimization\n",
    "## Prompt: Given some order fulfillment centers, how would you go about improving the efficiency of the fulfillment centers?\n",
    "\n",
    "#### Initial Thoughts\n",
    "I'd imagine we want to see how different features of a fulfillment center would affect the fulfillment time.\n",
    "\n",
    "#### Training Data:\n",
    "The input data would consist of the time of day the order fulfillment request was made, \n",
    "\n",
    "```\n",
    "Customer: How do I buy Bitcoin?\n",
    "CS representative: Here's a link to a support article on how to buy Bitcoin.\n",
    "Customer: Thanks!\n",
    "```\n",
    "\n",
    "Here we can assume the link provided was enough and the exchange ends. Thus our training data would comprise of these mappings between text input and an article selection output.\n",
    "\n",
    "\"How do I buy Bitcoin?\" → article 102 about buying Bitcoin on the app\n",
    "\"Where can I do X\" → article 59 about X\n",
    "...\n",
    "\n",
    "#### Approach\n",
    "Let's convert the text input into word vectors - the industry standard `word2vec` format will do.\n",
    "\n",
    "#### Models\n",
    "We can use a traditional Random Forest or we can use deep learning by building a large deep neural network.\n",
    "Either way, we would want to regularize because both tend to overfit if we are not careful.\n",
    "\n",
    "#### Output\n",
    "We can approach this by doing multinomial classification.\n",
    "\n",
    "#### Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
